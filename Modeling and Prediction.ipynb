{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Modeling and Prediction</h1>\n",
    "On the West Nile Virus data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading cleaned data\n",
    "X = pd.read_pickle('./data/train.pkl')\n",
    "y = pd.read_pickle('./data/y.pkl')\n",
    "#X_test = pd.read_pickle('./data/test.pkl')\n",
    "#y_test = pd.read_csv('./data/sampleSubmission.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping a few columns I mis-interpreted. Coding sprecies\n",
    "X = X.drop(['Trap','Sunrise_x','Sunset_x','Sunrise_y','Sunset_y'],axis=1)\n",
    "#X_test = X_test.drop(['Trap','Sunrise_x','Sunset_x','Sunrise_y','Sunset_y'],axis=1)\n",
    "\n",
    "#coding species columns\n",
    "X = pd.get_dummies(X,columns=['Species','month'])\n",
    "#X_test = pd.get_dummies(X_test,columns=['Species','month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistakenly, thought that the train_test_split was done for me. It was not, the test set is for the kaggle competition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Classifier</h2>\n",
    "Random Forest Classifier is a great ensemble method which I will look into first with a Random Search CV. The driving metric for analysis is recall. This data set is heavily class biased -- there are few west nile virus sightings compared to the observations as a whole. What's most important is that WNV is not missed, i.e. False Negative. For this purpose, a False Positive is better than False Negative, so recall is the most important metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Building Pipeline***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from scipy.stats import truncnorm, uniform\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline( \n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(), \n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'simpleimputer', 'standardscaler', 'randomforestclassifier', 'simpleimputer__add_indicator', 'simpleimputer__copy', 'simpleimputer__fill_value', 'simpleimputer__missing_values', 'simpleimputer__strategy', 'simpleimputer__verbose', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'randomforestclassifier__bootstrap', 'randomforestclassifier__ccp_alpha', 'randomforestclassifier__class_weight', 'randomforestclassifier__criterion', 'randomforestclassifier__max_depth', 'randomforestclassifier__max_features', 'randomforestclassifier__max_leaf_nodes', 'randomforestclassifier__max_samples', 'randomforestclassifier__min_impurity_decrease', 'randomforestclassifier__min_impurity_split', 'randomforestclassifier__min_samples_leaf', 'randomforestclassifier__min_samples_split', 'randomforestclassifier__min_weight_fraction_leaf', 'randomforestclassifier__n_estimators', 'randomforestclassifier__n_jobs', 'randomforestclassifier__oob_score', 'randomforestclassifier__random_state', 'randomforestclassifier__verbose', 'randomforestclassifier__warm_start'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'randomforestclassifier__n_estimators': np.arange(50,2000),\n",
    "         'randomforestclassifier__max_depth' :  np.arange(2,20)}\n",
    "\n",
    "rf_rand = RandomizedSearchCV(pipe,param_distributions=params,cv=5, n_jobs=-1, scoring = 'recall',n_iter=10,\n",
    "                            random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'randomforestclassifier__max_depth': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19]),\n",
       "                                        'randomforestclassifier__n_estimators': array([  50,   51,   52, ..., 1997, 1998, 1999])},\n",
       "                   random_state=42, scoring='recall')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Looking at results from the random search***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 562,\n",
       " 'randomforestclassifier__max_depth': 13}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res = pd.DataFrame(rf_rand.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_randomforestclassifier__n_estimators</th>\n",
       "      <th>param_randomforestclassifier__max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.783034</td>\n",
       "      <td>0.037325</td>\n",
       "      <td>0.247095</td>\n",
       "      <td>0.008149</td>\n",
       "      <td>562</td>\n",
       "      <td>13</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 562, ...</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.072289</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.084337</td>\n",
       "      <td>0.071888</td>\n",
       "      <td>0.019891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.367823</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.191029</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>473</td>\n",
       "      <td>10</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 473, ...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.059524</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.038296</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.692920</td>\n",
       "      <td>0.085785</td>\n",
       "      <td>0.540302</td>\n",
       "      <td>0.010560</td>\n",
       "      <td>1300</td>\n",
       "      <td>10</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 1300,...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.924767</td>\n",
       "      <td>0.026842</td>\n",
       "      <td>0.130939</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>245</td>\n",
       "      <td>10</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 245, ...</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.033505</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.426912</td>\n",
       "      <td>0.085862</td>\n",
       "      <td>0.549149</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>1584</td>\n",
       "      <td>7</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 1584,...</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.611328</td>\n",
       "      <td>0.221281</td>\n",
       "      <td>0.298769</td>\n",
       "      <td>0.063741</td>\n",
       "      <td>910</td>\n",
       "      <td>2</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 910, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.290562</td>\n",
       "      <td>0.245987</td>\n",
       "      <td>0.172204</td>\n",
       "      <td>0.033973</td>\n",
       "      <td>465</td>\n",
       "      <td>5</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 465, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.689578</td>\n",
       "      <td>0.064510</td>\n",
       "      <td>0.443207</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>1735</td>\n",
       "      <td>2</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 1735,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.614808</td>\n",
       "      <td>0.120915</td>\n",
       "      <td>0.196332</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>819</td>\n",
       "      <td>2</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 819, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.314372</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>0.091824</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>533</td>\n",
       "      <td>3</td>\n",
       "      <td>{'randomforestclassifier__n_estimators': 533, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "5       5.783034      0.037325         0.247095        0.008149   \n",
       "6       4.367823      0.099999         0.191029        0.005935   \n",
       "4      11.692920      0.085785         0.540302        0.010560   \n",
       "0       2.924767      0.026842         0.130939        0.003910   \n",
       "2      12.426912      0.085862         0.549149        0.003462   \n",
       "1       4.611328      0.221281         0.298769        0.063741   \n",
       "3       3.290562      0.245987         0.172204        0.033973   \n",
       "7       7.689578      0.064510         0.443207        0.006366   \n",
       "8       3.614808      0.120915         0.196332        0.018894   \n",
       "9       2.314372      0.064166         0.091824        0.001753   \n",
       "\n",
       "  param_randomforestclassifier__n_estimators  \\\n",
       "5                                        562   \n",
       "6                                        473   \n",
       "4                                       1300   \n",
       "0                                        245   \n",
       "2                                       1584   \n",
       "1                                        910   \n",
       "3                                        465   \n",
       "7                                       1735   \n",
       "8                                        819   \n",
       "9                                        533   \n",
       "\n",
       "  param_randomforestclassifier__max_depth  \\\n",
       "5                                      13   \n",
       "6                                      10   \n",
       "4                                      10   \n",
       "0                                      10   \n",
       "2                                       7   \n",
       "1                                       2   \n",
       "3                                       5   \n",
       "7                                       2   \n",
       "8                                       2   \n",
       "9                                       3   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "5  {'randomforestclassifier__n_estimators': 562, ...           0.071429   \n",
       "6  {'randomforestclassifier__n_estimators': 473, ...           0.047619   \n",
       "4  {'randomforestclassifier__n_estimators': 1300,...           0.047619   \n",
       "0  {'randomforestclassifier__n_estimators': 245, ...           0.047619   \n",
       "2  {'randomforestclassifier__n_estimators': 1584,...           0.011905   \n",
       "1  {'randomforestclassifier__n_estimators': 910, ...           0.000000   \n",
       "3  {'randomforestclassifier__n_estimators': 465, ...           0.000000   \n",
       "7  {'randomforestclassifier__n_estimators': 1735,...           0.000000   \n",
       "8  {'randomforestclassifier__n_estimators': 819, ...           0.000000   \n",
       "9  {'randomforestclassifier__n_estimators': 533, ...           0.000000   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "5           0.095238           0.072289           0.036145           0.084337   \n",
       "6           0.059524           0.024096           0.012048           0.048193   \n",
       "4           0.047619           0.024096           0.012048           0.048193   \n",
       "0           0.047619           0.024096           0.012048           0.036145   \n",
       "2           0.000000           0.012048           0.000000           0.000000   \n",
       "1           0.000000           0.000000           0.000000           0.000000   \n",
       "3           0.000000           0.000000           0.000000           0.000000   \n",
       "7           0.000000           0.000000           0.000000           0.000000   \n",
       "8           0.000000           0.000000           0.000000           0.000000   \n",
       "9           0.000000           0.000000           0.000000           0.000000   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "5         0.071888        0.019891                1  \n",
       "6         0.038296        0.017470                2  \n",
       "4         0.035915        0.015060                3  \n",
       "0         0.033505        0.013815                4  \n",
       "2         0.004791        0.005867                5  \n",
       "1         0.000000        0.000000                6  \n",
       "3         0.000000        0.000000                6  \n",
       "7         0.000000        0.000000                6  \n",
       "8         0.000000        0.000000                6  \n",
       "9         0.000000        0.000000                6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_res.sort_values('mean_test_score',axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0718875502008032"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest Results</h3>\n",
    "\n",
    "The highest recall score was about 7%, meaning that this model only predicts 7/100 WNV mosquitos correctly. This is wholly inadequate for any real-world application. I have a few thoughts on why performance is so bad:\n",
    "\n",
    "1. too many features. It's likely that with the large number of features I have included, many will covary and confuse the model. I will try PCA to reduce dimensionality. \n",
    "2. poor preprocessing. It's possible that there are underlying mistakes in how I cleaned and organized the data. A more experienced eye will have to look at it to tell me.\n",
    "3. Bad model. Maybe I'm asking random forest to do the wrong problem. It is my intention to try other models as well, like KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest with PCA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_PCA = make_pipeline( \n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    PCA(),\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'randomforestclassifier__n_estimators': np.arange(200,2000),\n",
    "         'randomforestclassifier__max_depth' :  np.arange(2,10),\n",
    "          'pca__n_components' : np.arange(5,75)}\n",
    "\n",
    "#starting with few iterations, and narrowing the ranges afterwards\n",
    "rf_rand_PCA = RandomizedSearchCV(pipe_PCA,param_distributions=params,cv=5, n_jobs=-1, \n",
    "                                 scoring = 'recall',n_iter=10,random_state = 42, verbose =10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   51.3s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  2.8min remaining:   37.0s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:  2.9min remaining:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('pca', PCA()),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'pca__n_components': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74]),\n",
       "                                        'randomforestclassifier__max_depth': array([2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                                        'randomforestclassifier__n_estimators': array([ 200,  201,  202, ..., 1997, 1998, 1999])},\n",
       "                   random_state=42, scoring='recall', verbose=10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_PCA.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 1978,\n",
       " 'randomforestclassifier__max_depth': 9,\n",
       " 'pca__n_components': 22}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_PCA.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04549627079747562"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_PCA.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA looks to have made the performance worse by almost a factor of two, so I will serach for better parameters for the non-pca approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('pca', PCA()),\n",
       "                                             ('randomforestclassifier',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'pca__n_components': array([45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61,\n",
       "       62, 63, 64]),\n",
       "                                        'randomforestcl...\n",
       "       1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438,\n",
       "       1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449,\n",
       "       1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460,\n",
       "       1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
       "       1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482,\n",
       "       1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493,\n",
       "       1494, 1495, 1496, 1497, 1498, 1499])},\n",
       "                   random_state=42, scoring='recall')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'randomforestclassifier__n_estimators': np.arange(900,1500),\n",
    "         'randomforestclassifier__max_depth' :  np.arange(4,10),\n",
    "          'pca__n_components' : np.arange(5,35)}\n",
    "\n",
    "#starting with few iterations, and narrowing the ranges afterwards\n",
    "rf_rand_PCA = RandomizedSearchCV(pipe_PCA,param_distributions=params,cv=5, n_jobs=-1, \n",
    "                                 scoring = 'recall',n_iter=50,random_state = 42)\n",
    "\n",
    "rf_rand_PCA.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__n_estimators': 1273,\n",
       " 'randomforestclassifier__max_depth': 9,\n",
       " 'pca__n_components': 54}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_PCA.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05031554790590935"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_PCA.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Precision Recall Curve - Random Forest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10506, 7879]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-87fc06cfd50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#finding curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_rf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcurve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'threshold'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \"\"\"\n\u001b[0;32m--> 675\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,\n\u001b[0m\u001b[1;32m    676\u001b[0m                                              \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                                              sample_weight=sample_weight)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10506, 7879]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "#saving estimator\n",
    "rf_best = rf_rand_PCA.best_estimator_\n",
    "\n",
    "#predicting from model\n",
    "y_pred_rf = rf_best.predict_proba(X_train)\n",
    "\n",
    "#finding curve\n",
    "p, r, t = precision_recall_curve(y.to_numpy(),y_pred_rf[:,1])\n",
    "\n",
    "curve = pd.DataFrame({'precision':p[:-1],'recall':r[:-1],'threshold':t})\n",
    "\n",
    "sns.lineplot(x=p,y=r)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve[np.round(curve.recall,2) == 0.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest Conclusions</h3>\n",
    "\n",
    "On the training set, at a threshold of 15%, the random forrest regressor with 1393 estimators, max depth of 9, and 52 PCA components does quite well. It yields roughly 80% recall and 50% precision. In more direct terms, roughly 8/10 WNV mosquitos will be correctly predicted as so, while about 50% of predicted WNV will be accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Set Metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def member_by_thresh(y,t):\n",
    "    return (y >= t).astype('int')\n",
    "\n",
    "#grabbing best estimator\n",
    "rf_best = rf_rand_PCA.best_estimator_\n",
    "\n",
    "#predicting probability from test set\n",
    "y_pred = rf_best.predict_proba(X_test)[:,1]\n",
    "y_pred = member_by_thresh(y_pred,0.15) \n",
    "\n",
    "#grabbing true group\n",
    "y_true = y_test.WnvPresent\n",
    "\n",
    "print('Evaluating the random forest classifier on the test set, the recall is: ' +  \n",
    "      '{}, and the precision is: {}'.format(recall_score(y_true,y_pred),precision_score(y_true,y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression</h2>\n",
    "Logistic Regression might be the right classifier for this application. Given that I would like to maximize recall, adjusting the barrier could be a great way to do so. I will take a similar approach of random search over large variety with few iterations, and then narrow down and do more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline( \n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    PCA(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "params = {'logisticregression__C': np.logspace(2,6),\n",
    "          'pca__n_components' : np.arange(5,70)}\n",
    "\n",
    "#starting with few iterations, and narrowing the ranges afterwards\n",
    "rf_rand_lr = RandomizedSearchCV(pipe_lr,param_distributions=params,cv=5, n_jobs=-1, \n",
    "                                 scoring = 'recall',n_iter=5,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('pca', PCA()),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression(max_iter=1000))]),\n",
       "                   n_iter=5, n_jobs=-1,\n",
       "                   param_distributions={'logisticregression__C': array([1.00000000e+02, 1.20679264e+02, 1.45634848e+02, 1.75751062e+02,\n",
       "       2.1209508...\n",
       "       1.84206997e+05, 2.22299648e+05, 2.68269580e+05, 3.23745754e+05,\n",
       "       3.90693994e+05, 4.71486636e+05, 5.68986603e+05, 6.86648845e+05,\n",
       "       8.28642773e+05, 1.00000000e+06]),\n",
       "                                        'pca__n_components': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69])},\n",
       "                   random_state=42, scoring='recall')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__n_components': 59, 'logisticregression__C': 828642.7728546843}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14484848484848484"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('simpleimputer',\n",
       "                                              SimpleImputer(strategy='median')),\n",
       "                                             ('standardscaler',\n",
       "                                              StandardScaler()),\n",
       "                                             ('pca', PCA()),\n",
       "                                             ('logisticregression',\n",
       "                                              LogisticRegression(max_iter=1000))]),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'logisticregression__C': array([1.00000000e+02, 1.20679264e+02, 1.45634848e+02, 1.75751062e+02,\n",
       "       2.120950...\n",
       "       4.09491506e+04, 4.94171336e+04, 5.96362332e+04, 7.19685673e+04,\n",
       "       8.68511374e+04, 1.04811313e+05, 1.26485522e+05, 1.52641797e+05,\n",
       "       1.84206997e+05, 2.22299648e+05, 2.68269580e+05, 3.23745754e+05,\n",
       "       3.90693994e+05, 4.71486636e+05, 5.68986603e+05, 6.86648845e+05,\n",
       "       8.28642773e+05, 1.00000000e+06]),\n",
       "                                        'pca__n_components': array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
       "       67, 68, 69])},\n",
       "                   random_state=42, scoring='recall')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr = make_pipeline( \n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(),\n",
    "    PCA(),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "params = {'logisticregression__C': np.logspace(2,6),\n",
    "          'pca__n_components' : np.arange(50,70)}\n",
    "\n",
    "#starting with few iterations, and narrowing the ranges afterwards\n",
    "rf_rand_lr = RandomizedSearchCV(pipe_lr,param_distributions=params,cv=5, n_jobs=-1, \n",
    "                                 scoring = 'recall',n_iter=50,random_state = 42)\n",
    "\n",
    "rf_rand_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__n_components': 63, 'logisticregression__C': 828642.7728546843}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14666666666666664"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rand_lr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Precision Recall Curve - Logistic Regression</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving estimator\n",
    "lr_best = rf_rand_lr.best_estimator_\n",
    "\n",
    "#predicting from model\n",
    "y_pred_rf = lr_best.predict_proba(X)\n",
    "\n",
    "#finding curve\n",
    "p, r, t = precision_recall_curve(y.to_numpy(),y_pred_rf[:,1])\n",
    "\n",
    "curve = pd.DataFrame({'precision':p[:-1],'recall':r[:-1],'threshold':t})\n",
    "\n",
    "sns.lineplot(x=p,y=r)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
